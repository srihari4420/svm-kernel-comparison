{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8ed1667",
   "metadata": {},
   "source": [
    "# SVM Kernel Comparison — Tutorial + Experiments\n",
    "\n",
    "Understanding SVM Kernel Choices: Linear, RBF, and Polynomial\n",
    "\n",
    "Introduction\n",
    "Support Vector Machines (SVMs) are supervised learning models that find a separating hyperplane between classes.\n",
    "When data are not linearly separable, kernels let SVMs operate implicitly in higher-dimensional spaces so they can\n",
    "still form linear separators in that transformed space. This tutorial explains three common kernels — Linear, RBF,\n",
    "and Polynomial — and demonstrates their behaviour with visual examples and practical guidance for selection.\n",
    "\n",
    "Why kernels matter\n",
    "A kernel computes similarity between two points. Choosing a kernel determines the kinds of decision boundaries the\n",
    "SVM can represent. The Linear kernel produces flat hyperplanes; the Polynomial kernel creates polynomial-shaped\n",
    "boundaries controlled by degree; the RBF kernel produces local, flexible boundaries controlled by the gamma parameter.\n",
    "\n",
    "Datasets used for illustration\n",
    "We use three synthetic 2D datasets that highlight different challenges:\n",
    "- Moons: two interleaving half-circles (nonlinear, curved).\n",
    "- Circles: concentric circles (highly nonlinear).\n",
    "- Blobs: Gaussian clusters (approximately linear separability).\n",
    "These datasets allow clear visual comparisons of kernel behaviour.\n",
    "\n",
    "Practical pipeline and hyperparameters\n",
    "To ensure fair comparison, each SVM is trained inside a scikit-learn Pipeline that scales features using StandardScaler.\n",
    "Important hyperparameters:\n",
    "- C (regularization): larger C → less regularization → potentially more complex boundary.\n",
    "- gamma (RBF): small gamma → smoother decision boundary; large gamma → more wiggly, possibly overfit.\n",
    "- degree, coef0 (polynomial): control polynomial shape and offset.\n",
    "We use GridSearchCV with 5-fold stratified cross-validation to select hyperparameters for each kernel independently.\n",
    "\n",
    "Visualising decision boundaries and support vectors\n",
    "For each dataset and kernel, decision regions are plotted on a dense grid and the learned support vectors are shown.\n",
    "This provides an intuitive view of how flexible each kernel is and how support vectors concentrate near decision boundaries.\n",
    "\n",
    "Key observations (expected)\n",
    "- Linear kernel: performs well on blobs (linearly separable) and is fastest; fails on moons/circles.\n",
    "- RBF kernel: very flexible and often performs best on nonlinear datasets; sensitive to gamma and C.\n",
    "- Polynomial kernel: capable of curved boundaries; degree must be chosen carefully to avoid overfitting.\n",
    "\n",
    "Recommendations / Rules of thumb\n",
    "- Start with a Linear kernel for high-dimensional, sparse data.\n",
    "- Use RBF as a strong default for low-dimensional nonlinear problems; tune gamma using cross-validation.\n",
    "- Use Polynomial when you suspect polynomial relationships or interactions; keep degree small (2–3) unless justified.\n",
    "- Always scale features before SVM training and use cross-validation to choose C and other hyperparameters.\n",
    "\n",
    "Ethical considerations\n",
    "Kernel choice affects accuracy and, in turn, downstream fairness and decisions. Poor model selection may harm\n",
    "stakeholders; therefore document choices, test across subgroups, and report limitations transparently.\n",
    "\n",
    "References\n",
    "- Cortes, C. and Vapnik, V. (1995). Support-vector networks.\n",
    "- scikit-learn documentation: https://scikit-learn.org/stable/modules/svm.html\n",
    "- Schölkopf, B. & Smola, A. J. (2001). Learning with Kernels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f277b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports and setup\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, os, joblib\n",
    "from sklearn.datasets import make_moons, make_circles, make_blobs\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "np.random.seed(0)\n",
    "OUTDIR = \"/mnt/data/svm_submission_package/artifacts\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "PALETTE = ['#0072B2', '#D55E00']  # colorblind-friendly pair (blue, orange)\n",
    "plt.rcParams['figure.figsize'] = (6,4)\n",
    "print(\"Setup complete. Artifacts directory:\", OUTDIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68ce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate datasets\n",
    "RANDOM_STATE = 0\n",
    "datasets = {\n",
    "    'moons': make_moons(n_samples=300, noise=0.2, random_state=RANDOM_STATE),\n",
    "    'circles': make_circles(n_samples=300, noise=0.1, factor=0.5, random_state=RANDOM_STATE),\n",
    "    'blobs': make_blobs(n_samples=300, centers=2, cluster_std=1.0, random_state=RANDOM_STATE)\n",
    "}\n",
    "for name,(X,y) in datasets.items():\n",
    "    plt.figure(); plt.scatter(X[:,0], X[:,1], c=y, edgecolors='k', s=40); plt.title(f\"Dataset: {name}\"); plt.xlabel('x0'); plt.ylabel('x1'); plt.tight_layout(); plt.savefig(os.path.join(OUTDIR,f\"dataset_{name}.png\")); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd4d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "def plot_decision_boundary_and_support(clf, X, y, title, savepath=None):\n",
    "    x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n",
    "    y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 400), np.linspace(y_min, y_max, 400))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = clf.predict(grid).reshape(xx.shape)\n",
    "    cmap = ListedColormap(PALETTE)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.2, cmap=cmap)\n",
    "    plt.scatter(X[:,0], X[:,1], c=y, edgecolors='k', s=40)\n",
    "    svc = None\n",
    "    if hasattr(clf, 'named_steps'):\n",
    "        for step in clf.named_steps.values():\n",
    "            if hasattr(step, 'support_'):\n",
    "                svc = step\n",
    "    else:\n",
    "        if hasattr(clf, 'support_'): svc = clf\n",
    "    if svc is not None:\n",
    "        sv = svc.support_vectors_\n",
    "        plt.scatter(sv[:,0], sv[:,1], facecolors='none', edgecolors='k', s=100, linewidths=1.2)\n",
    "    plt.title(title); plt.xlabel('x0'); plt.ylabel('x1'); plt.tight_layout()\n",
    "    if savepath: plt.savefig(savepath)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f1fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grids = {\n",
    "    'linear': {'svc__C': [0.01,0.1,1,10,100], 'svc__kernel':['linear']},\n",
    "    'rbf': {'svc__C': [0.1,1,10,100], 'svc__gamma': ['scale',0.1,1,5], 'svc__kernel':['rbf']},\n",
    "    'poly': {'svc__C': [0.1,1,10], 'svc__degree': [2,3], 'svc__gamma': ['scale',0.1,1], 'svc__coef0':[0.0,1.0], 'svc__kernel':['poly']}\n",
    "}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad28dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "for dname,(X,y) in datasets.items():\n",
    "    print('\\n'+'='*50); print('Dataset:', dname)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, stratify=y, random_state=RANDOM_STATE)\n",
    "    for kname, grid in param_grids.items():\n",
    "        print('\\n-- Kernel:', kname)\n",
    "        pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC(random_state=RANDOM_STATE))])\n",
    "        gs = GridSearchCV(pipe, param_grid=grid, cv=cv, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "        gs.fit(X_train, y_train)\n",
    "        best = gs.best_estimator_\n",
    "        test_acc = best.score(X_test, y_test)\n",
    "        print('Best params:', gs.best_params_, ' | CV acc:', gs.best_score_, ' | Test acc:', test_acc)\n",
    "        model_path = os.path.join(OUTDIR, f'model_{dname}_{kname}.joblib')\n",
    "        joblib.dump(gs, model_path)\n",
    "        fig_path = os.path.join(OUTDIR, f'boundary_{dname}_{kname}.png')\n",
    "        plot_decision_boundary_and_support(best, X, y, title=f'{dname} - {kname} (best)', savepath=fig_path)\n",
    "        y_pred = best.predict(X_test)\n",
    "        print(classification_report(y_test, y_pred, digits=3))\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(); plt.title(f'Confusion: {dname} - {kname}'); plt.show()\n",
    "        results.append({'dataset':dname,'kernel':kname,'best_params':gs.best_params_,'cv_acc':float(gs.best_score_),'test_acc':float(test_acc),'model':model_path,'figure':fig_path})\n",
    "results_df = pd.DataFrame(results).sort_values(['dataset','test_acc'], ascending=[True,False]).reset_index(drop=True)\n",
    "results_df.to_csv(os.path.join(OUTDIR,'results_summary.csv'), index=False)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245beac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sensitivity plots for moons dataset\n",
    "X_m, y_m = datasets['moons']\n",
    "linear_C = [0.01,0.1,1,10,100]\n",
    "linear_scores = []\n",
    "for C in linear_C:\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC(kernel='linear', C=C, random_state=RANDOM_STATE))])\n",
    "    linear_scores.append(cross_val_score(pipe, X_m, y_m, cv=cv, scoring='accuracy', n_jobs=-1).mean())\n",
    "plt.figure(); plt.plot(linear_C, linear_scores, marker='o'); plt.xscale('log'); plt.xlabel('C'); plt.ylabel('CV acc'); plt.title('Linear: C effect (moons)'); plt.tight_layout(); plt.savefig(os.path.join(OUTDIR,'linear_C_effect.png')); plt.show()\n",
    "\n",
    "rbf_gamma = [0.01,0.1,1,5,10]\n",
    "rbf_scores = []\n",
    "for g in rbf_gamma:\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC(kernel='rbf', C=1, gamma=g, random_state=RANDOM_STATE))])\n",
    "    rbf_scores.append(cross_val_score(pipe, X_m, y_m, cv=cv, scoring='accuracy', n_jobs=-1).mean())\n",
    "plt.figure(); plt.plot(rbf_gamma, rbf_scores, marker='o'); plt.xscale('log'); plt.xlabel('gamma'); plt.ylabel('CV acc'); plt.title('RBF: gamma effect (moons)'); plt.tight_layout(); plt.savefig(os.path.join(OUTDIR,'rbf_gamma_effect.png')); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d4e87a",
   "metadata": {},
   "source": [
    "## How to run\n",
    "\n",
    "1. Install: `pip install scikit-learn matplotlib pandas joblib reportlab`\n",
    "2. Open `svm_kernel_comparison_full.ipynb` and run all cells.\n",
    "3. Artifacts (models, figures, CSV) will be saved to the `artifacts/` folder inside the package.\n",
    "\n",
    "Files included:\n",
    "- svm_kernel_tutorial_polished.pdf\n",
    "- svm_kernel_comparison_full.ipynb\n",
    "- artifacts/ (generated when notebook is run)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
